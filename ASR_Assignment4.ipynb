{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT - 4\n",
    "# NAME :ARVIND SHANKAR RAMAN\n",
    "# NET-ID : AXS170059"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION:\n",
    "### BUAN6340 Programming for Data Science (Python)\n",
    "### 1.\tPlease download spam.csv from eLearning.\n",
    "### 2.\tTrain a Naïve Bayes model to predict spam.\n",
    "### 3.\tReport the accuracy rate for your training and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "spam = pd.read_csv('spam.csv', encoding='ISO-8859-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.drop(spam.columns[2:5], axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "v1    5572 non-null object\n",
      "v2    5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n"
     ]
    }
   ],
   "source": [
    "spam.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the spam v2 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam['v2']=spam['v2'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the tokenized words in to the bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bag_of_words_features(words):\n",
    "    return {word:True for word in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam['v2'] = spam.v2.apply(lambda x : (build_bag_of_words_features(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords and creating a filtered bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91909\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam['v2'] = spam.v2.apply(lambda row : [stemmer.stem(word) for word in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go',\n",
       " 'until',\n",
       " 'jurong',\n",
       " 'point',\n",
       " ',',\n",
       " 'crazy..',\n",
       " 'avail',\n",
       " 'onli',\n",
       " 'in',\n",
       " 'bugi',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " '...',\n",
       " 'cine',\n",
       " 'there',\n",
       " 'got',\n",
       " 'amor',\n",
       " 'wat']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam['v2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bag_of_words_features_filtered(words):\n",
    "    return {word:1 for word in words if not word in useless_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam['v2'] = spam.v2.apply(lambda x : (build_bag_of_words_features_filtered(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Go': 1,\n",
       " 'jurong': 1,\n",
       " 'point': 1,\n",
       " 'crazy..': 1,\n",
       " 'avail': 1,\n",
       " 'onli': 1,\n",
       " 'bugi': 1,\n",
       " 'n': 1,\n",
       " 'great': 1,\n",
       " 'world': 1,\n",
       " 'la': 1,\n",
       " 'e': 1,\n",
       " 'buffet': 1,\n",
       " '...': 1,\n",
       " 'cine': 1,\n",
       " 'got': 1,\n",
       " 'amor': 1,\n",
       " 'wat': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.v2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the filtered column into positive features and negative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(spam)):\n",
    "    if(spam['v1'][i] == 'ham'):\n",
    "        spam['v2'][i]=(spam['v2'][i],'ham')\n",
    "    if(spam['v1'][i] == 'spam'):\n",
    "        spam['v2'][i]=(spam['v2'][i],'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_features=spam['v2'][spam['v1']=='ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_features=spam['v2'][spam['v1']=='spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4825"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_features)\n",
    "type(positive_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_features)\n",
    "type(negative_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ({'Go': 1, 'jurong': 1, 'point': 1, 'crazy..':...\n",
       "1       ({'Ok': 1, 'lar': 1, '...': 1, 'joke': 1, 'wif...\n",
       "3       ({'U': 1, 'dun': 1, 'say': 1, 'earli': 1, 'hor...\n",
       "4       ({'nah': 1, 'I': 1, 'n't': 1, 'think': 1, 'goe...\n",
       "6       ({'even': 1, 'brother': 1, 'like': 1, 'speak':...\n",
       "7       ({'As': 1, 'per': 1, 'request': 1, ''mell': 1,...\n",
       "10      ({'I': 1, ''m': 1, 'gon': 1, 'na': 1, 'home': ...\n",
       "13      ({'I': 1, ''ve': 1, 'search': 1, 'right': 1, '...\n",
       "14      ({'I': 1, 'A': 1, 'date': 1, 'ON': 1, 'sunday'...\n",
       "16      ({'Oh': 1, 'k': 1, '...': 1, ''m': 1, 'watch':...\n",
       "17      ({'Eh': 1, 'u': 1, 'rememb': 1, '2': 1, 'spell...\n",
       "18      ({'fine': 1, 'thatåõ': 1, 'way': 1, 'u': 1, 'f...\n",
       "20      ({'Is': 1, 'serious': 1, 'spell': 1, 'hi': 1, ...\n",
       "21      ({'iû÷m': 1, 'go': 1, 'tri': 1, '2': 1, 'mont...\n",
       "22      ({'So': 1, 'Ì_': 1, 'pay': 1, 'first': 1, 'lar...\n",
       "23      ({'aft': 1, 'finish': 1, 'lunch': 1, 'go': 1, ...\n",
       "24      ({'ffffffffff': 1, 'alright': 1, 'way': 1, 'I'...\n",
       "25      ({'forc': 1, 'eat': 1, 'slice': 1, 'I': 1, ''m...\n",
       "26            ({'lol': 1, 'alway': 1, 'convinc': 1}, ham)\n",
       "27      ({'catch': 1, 'bu': 1, 'fri': 1, 'egg': 1, 'ma...\n",
       "28      ({'I': 1, ''m': 1, 'back': 1, 'amp': 1, ''re':...\n",
       "29      ({'ahhh': 1, 'work': 1, 'I': 1, 'vagu': 1, 're...\n",
       "30      ({'wait': 1, ''s': 1, 'still': 1, 'clear': 1, ...\n",
       "31      ({'yeah': 1, 'got': 1, '2': 1, 'wa': 1, 'v': 1...\n",
       "32                 ({'K': 1, 'tell': 1, 'anyth': 1}, ham)\n",
       "33      ({'fear': 1, 'faint': 1, 'housework': 1, 'quic...\n",
       "35      ({'yup': 1, '...': 1, 'Ok': 1, 'go': 1, 'home'...\n",
       "36      ({'oop': 1, 'I': 1, ''ll': 1, 'let': 1, 'know'...\n",
       "37      ({'I': 1, 'see': 1, 'letter': 1, 'B': 1, 'car'...\n",
       "38      ({'anyth': 1, 'lor': 1, '...': 1, 'U': 1, 'dec...\n",
       "                              ...                        \n",
       "5342    ({'u': 1, 'r': 1, 'subscrib': 1, '2': 1, 'text...\n",
       "5364    ({'call': 1, '09095350301': 1, 'send': 1, 'gir...\n",
       "5365    ({'camera': 1, 'award': 1, 'sipix': 1, 'digit'...\n",
       "5366    ({'A': 1, 'å£400': 1, 'xma': 1, 'reward': 1, '...\n",
       "5368    ({'import': 1, 'messag': 1, 'thi': 1, 'final':...\n",
       "5370    ({'date': 1, 'two': 1, 'onli': 1, 'start': 1, ...\n",
       "5377    ({'current': 1, 'lead': 1, 'bid': 1, '151': 1,...\n",
       "5378    ({'free': 1, 'entri': 1, 'gr8prize': 1, 'wkli'...\n",
       "5381    ({'1': 1, 'new': 1, 'messag': 1, 'call': 1, '0...\n",
       "5427    ({'santa': 1, 'call': 1, 'would': 1, 'littl': ...\n",
       "5443    ({'guarante': 1, '32000': 1, 'award': 1, 'mayb...\n",
       "5449    ({'latest': 1, 'new': 1, 'polic': 1, 'station'...\n",
       "5456    ({'\\for': 1, 'sparkl': 1, 'shop': 1, 'break': ...\n",
       "5460    ({'decemb': 1, 'onli': 1, 'mobil': 1, '11mths+...\n",
       "5462    ({'txt': 1, 'call': 1, 'No': 1, '86888': 1, 'c...\n",
       "5466    ({'http//tm': 1, 'widelive.com/index': 1, 'wml...\n",
       "5467    ({'get': 1, 'garden': 1, 'readi': 1, 'summer':...\n",
       "5468    ({'urgent': 1, 'last': 1, 'weekend': 1, ''s': ...\n",
       "5482    ({'urgent': 1, 'We': 1, 'tri': 1, 'contact': 1...\n",
       "5487    ({'2p': 1, 'per': 1, 'min': 1, 'call': 1, 'ger...\n",
       "5492    ({'marvel': 1, 'mobil': 1, 'play': 1, 'offici'...\n",
       "5497    ({'sm': 1, 'servic': 1, 'inclus': 1, 'text': 1...\n",
       "5501    ({'privat': 1, '2003': 1, 'account': 1, 'state...\n",
       "5524    ({'award': 1, 'sipix': 1, 'digit': 1, 'camera'...\n",
       "5526    ({'privat': 1, '2003': 1, 'account': 1, 'state...\n",
       "5537    ({'want': 1, 'explicit': 1, 'sex': 1, '30': 1,...\n",
       "5540    ({'ask': 1, '3mobil': 1, 'IF': 1, '0870': 1, '...\n",
       "5547    ({'contract': 1, 'mobil': 1, '11': 1, 'mnth': ...\n",
       "5566    ({'remind': 1, 'O2': 1, 'To': 1, 'get': 1, '2....\n",
       "5567    ({'thi': 1, '2nd': 1, 'time': 1, 'tri': 1, '2'...\n",
       "Name: v2, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([positive_features, negative_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnning the Naive Bayes Classifier Model on the positive feautres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4457"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = int(0.8*len(spam))\n",
    "split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier = NaiveBayesClassifier.train(pd.concat([positive_features[:split], negative_features[:split]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'Go': 1, 'jurong': 1, 'point': 1, 'crazy..': 1, 'avail': 1, 'onli': 1, 'bugi': 1, 'n': 1, 'great': 1, 'world': 1, 'la': 1, 'e': 1, 'buffet': 1, '...': 1, 'cine': 1, 'got': 1, 'amor': 1, 'wat': 1}, 'ham')\n"
     ]
    }
   ],
   "source": [
    "print(positive_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the training accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.48501152959263"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.util.accuracy(sentiment_classifier, pd.concat([positive_features[:split], negative_features[:split]]))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the testing accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.3913043478261"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.util.accuracy(sentiment_classifier, pd.concat([positive_features[split:], negative_features[split:]]))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training accuracy rate obtained  is 94.48\n",
    "#### The testing accuracy rate obtained is  92.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   award = 1                spam : ham    =    228.5 : 1.0\n",
      "                 voucher = 1                spam : ham    =    141.1 : 1.0\n",
      "                    code = 1                spam : ham    =    117.2 : 1.0\n",
      "                   await = 1                spam : ham    =    109.3 : 1.0\n",
      "                  urgent = 1                spam : ham    =    104.7 : 1.0\n",
      "                      16 = 1                spam : ham    =     97.3 : 1.0\n",
      "                   nokia = 1                spam : ham    =     89.4 : 1.0\n",
      "                 landlin = 1                spam : ham    =     82.2 : 1.0\n",
      "                     txt = 1                spam : ham    =     82.1 : 1.0\n",
      "                deliveri = 1                spam : ham    =     81.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "sentiment_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "### 1. The spam.csv was downloaded and the Naive Bayes model was trained to predict spam.\n",
    "### 2. The training accuracy rate obtained  is 94.48 and the testing accuracy rate obtained is  92.39.\n",
    "### 3. Also, the top 3 most_informative_features were award, voucher and code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
